{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 运行文件test_gpy.py,查看GPU的配置信息\n",
    "!python test_gpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.4 Numpy与Tensor</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.1  torch概述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1,2])\n",
    "y = torch.tensor([3,4])\n",
    "# 不修改自身数据\n",
    "z = x.add(y)\n",
    "print(z)\n",
    "print(x)\n",
    "# 修改自身数据\n",
    "x.add_(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 创建tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 根据list数据生成tensor\n",
    "torch.Tensor([1,2,3,4,5,6])\n",
    "# 根据指定形状生成tensor\n",
    "torch.Tensor(2,3)\n",
    "# 根据给定的tensor的形状\n",
    "t=torch.Tensor([[1,2,3],[4,5,6]])\n",
    "# 查看tensor的形状\n",
    "print(t.size())\n",
    "# shape与size()等价方式\n",
    "print(t.shape)\n",
    "# 根据已有形状创建tensor\n",
    "print(torch.Tensor(t.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# torch.tensor与torch.Tensor的区别\n",
    "import torch\n",
    "t1 = torch.Tensor(1)\n",
    "t2 = torch.tensor(1)\n",
    "print(\"t1的值{},t1的数据类型{}\".format(t1,t1.type()))\n",
    "print(\"t2的值{},t2的数据类型{}\".format(t2,t2.type()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#根据一定规则，自动生成tensor的一些例子\n",
    "import torch\n",
    "\n",
    "# 生成一个单位矩阵\n",
    "print(torch.eye(2,2))\n",
    "# 自动生成全是0的矩阵\n",
    "print(torch.zeros(2,3))\n",
    "# 根据规则生成数据\n",
    "print(torch.linspace(1,10,4))\n",
    "# 生成满足均匀分布随机数\n",
    "print(torch.rand(2,3))\n",
    "# 生成满足标准分布随机数\n",
    "print(torch.randn(2,3))\n",
    "# 返回所给数据形状相同，值全为0的张量\n",
    "print(torch.zeros_like(torch.rand(2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 修改tensor的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 生成一个形状为2x3的矩阵\n",
    "x = torch.randn(2, 3)\n",
    "# 查看矩阵的形状\n",
    "x.size()   # 结果为torch.Size([2, 3])\n",
    "\n",
    "# 查看x的维度\n",
    "x.dim()    # 结果为2\n",
    "# 把x变为3x2的矩阵\n",
    "x.view(3,2)\n",
    "# 把x展平为1维向量\n",
    "y = x.view(-1)\n",
    "print(y.shape)\n",
    "# 添加一个维度\n",
    "z = torch.unsqueeze(y,0)\n",
    "# 查看z的形状\n",
    "z.size()   # 结果为torch.Size([1, 6])\n",
    "# 计算Z的元素个数\n",
    "z.numel()   # 结果为6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.4 索引操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 设置一个随机种子\n",
    "torch.manual_seed(100) \n",
    "# 生成一个形状为2x3的矩阵\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "# 根据索引获取第1行，所有数据\n",
    "print(x[0,:])\n",
    "# 获取最后一列数据\n",
    "print(x[:,-1])\n",
    "# 生成是否大于0的Byter张量\n",
    "mask = x>0\n",
    "print(mask)\n",
    "# 获取大于0的值\n",
    "print(torch.masked_select(x,mask))\n",
    "# 获取非0下标,即行，列索引\n",
    "print(torch.nonzero(mask))\n",
    "# 获取指定索引对应的值,输出根据以下规则得到\n",
    "# out[i][j] = input[index[i][j]][j]  # if dim == 0\n",
    "# out[i][j] = input[i][index[i][j]]  # if dim == 1\n",
    "index = torch.LongTensor([[0,1,1]])\n",
    "torch.gather(x,0,index)\n",
    "index = torch.LongTensor([[0,1,1],[1,1,1]])\n",
    "a = torch.gather(x,1,index)\n",
    "# 把a的值返回到一个2x3的0矩阵中\n",
    "z = torch.zeros(2,3)\n",
    "z.scatter_(1,index,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.5 广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "A = np.arange(0, 40,10).reshape(4, 1)\n",
    "B = np.arange(0, 3)\n",
    "# 把ndarray转换为Tensor\n",
    "A1 = torch.from_numpy(A)  #形状为4x1\n",
    "B1 = torch.from_numpy(B)  #形状为3\n",
    "# Tensor自动实现广播\n",
    "C = A1+B1\n",
    "# 我们可以根据广播机制，手工进行配置\n",
    "# 根据规则1，B1需要向A1看齐，把B变为（1,3）\n",
    "B2 = B1.unsqueeze(0)  #B2的形状为1x3\n",
    "# 使用expand函数重复数组，分别的4x3的矩阵\n",
    "A2 = A1.expand(4,3)\n",
    "B3 = B2.expand(4,3)\n",
    "# 然后进行相加,C1与C结果一致\n",
    "C1 = A2+B3\n",
    "print(C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.6 遂元操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.randn(1, 3)\n",
    "t1 = torch.randn(3, 1)\n",
    "t2 = torch.randn(1, 3)\n",
    "# t+0.1*(t1/t2)\n",
    "torch.addcdiv(t, 0.1, t1, t2)\n",
    "# 计算sigmoid\n",
    "torch.sigmoid(t)\n",
    "# 将t限制在[0,1]之间\n",
    "torch.clamp(t,0,1)\n",
    "# t+2进行就地运算\n",
    "t.add_(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.7 归并操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 生成一个含6个数的向量\n",
    "a = torch.linspace(0,10,6)\n",
    "# 使用view方法，把a变为2x3矩阵\n",
    "a = a.view((2,3))\n",
    "# 沿y轴方向累加，即dim=0\n",
    "b = a.sum(dim=0)   #b的形状为[3]\n",
    "# 沿y轴方向累加，即dim=0,并保留含1的维度\n",
    "b = a.sum(dim=0,keepdim=True) #b的形状为[1,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.8 比较操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.linspace(0,10,6).view(2,3)\n",
    "# 求所有元素的最大值\n",
    "torch.max(x)   # 结果为10\n",
    "# 求y轴方向的最大值\n",
    "torch.max(x,dim=0)  # 结果为[6,8,10]\n",
    "# 求最大的2个元素\n",
    "torch.topk(x,1,dim=0)  # 结果为[6,8,10],对应索引为tensor([[1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.9 矩阵操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2, 3])\n",
    "b = torch.tensor([3, 4])\n",
    "\n",
    "torch.dot(a,b)  #运行结果为18\n",
    "x = torch.randint(10,(2,3))\n",
    "y = torch.randint(6,(3,4))\n",
    "torch.mm(x,y)\n",
    "x = torch.randint(10,(2,2,3))\n",
    "y = torch.randint(6,(2,3,4))\n",
    "torch.bmm(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.5 Tensor与Autograd</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2.5.3 标量反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 定义输入张量x\n",
    "x = torch.Tensor([2])\n",
    "# 初始化权重参数W,偏移量b、并设置require_grad为True，为自动求导\n",
    "w = torch.randn(1,requires_grad=True)\n",
    "b = torch.randn(1,requires_grad=True)\n",
    "# 实现前向传播\n",
    "y = torch.mul(w,x)  # 等价于w*x\n",
    "z = torch.add(y,b)  # 等价于y+b\n",
    "# 查看x,w，b页子节点的requite_grad属性\n",
    "print(\"x,w,b的require_grad属性分别为：{},{},{}\".format(x.requires_grad,w.requires_grad,b.requires_grad))\n",
    "# 查看非叶子节点的requres_grad属性,\n",
    "print(\"y，z的requires_grad属性分别为：{},{}\".format(y.requires_grad,z.requires_grad))\n",
    "# 因与w，b有依赖关系，故y，z的requires_grad属性也是：True,True\n",
    "# 查看各节点是否为叶子节点\n",
    "print(\"x，w，b，y，z的是否为叶子节点：{},{},{},{},{}\".format(x.is_leaf,w.is_leaf,b.is_leaf,y.is_leaf,z.is_leaf))\n",
    "# x，w，b，y，z的是否为叶子节点：True,True,True,False,False\n",
    "# 查看叶子节点的grad_fn属性\n",
    "print(\"x，w，b的grad_fn属性：{},{},{}\".format(x.grad_fn,w.grad_fn,b.grad_fn))\n",
    "# 因x，w，b为用户创建的，为通过其他张量计算得到，故x，w，b的grad_fn属性：None,None,None\n",
    "# 查看非叶子节点的grad_fn属性\n",
    "print(\"y，z的是否为叶子节点：{},{}\".format(y.grad_fn,z.grad_fn))\n",
    "# y，z的是否为叶子节点：<MulBackward0 object at 0x7f923e85dda0>,<AddBackward0 object at 0x7f923e85d9b0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 基于z张量进行梯度反向传播,执行backward之后计算图会自动清空，\n",
    "# 如果需要多次使用backward，需要修改参数retain_graph为True，此时梯度是累加的\n",
    "# z.backward(retain_graph=True)\n",
    "z.backward()\n",
    "# 查看叶子节点的梯度，x是叶子节点但它无需求导，故其梯度为None\n",
    "print(\"参数w,b的梯度分别为:{},{},{}\".format(w.grad,b.grad,x.grad))\n",
    "# 参数w,b的梯度分别为:tensor([2.]),tensor([1.]),None\n",
    "\n",
    "# 非叶子节点的梯度，执行backward之后，会自动清空\n",
    "print(\"非叶子节点y,z的梯度分别为:{},{}\".format(y.grad,z.grad))\n",
    "# 非叶子节点y,z的梯度分别为:None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.4 非标量反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# backward函数的格式为：\n",
    "# backward(gradient=None, retain_graph=None, create_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 定义叶子节点张量x，形状为1x2\n",
    "x = torch.tensor([[2, 3]], dtype=torch.float, requires_grad=True)\n",
    "# 初始化Jacobian矩阵\n",
    "J = torch.zeros(2 ,2)\n",
    "# 初始化目标张量，形状为1x2\n",
    "y = torch.zeros(1, 2)\n",
    "# 定义y与x之间的映射关系：\n",
    "# y1=x1**2+3*x2，y2=x2**2+2*x1\n",
    "y[0, 0] = x[0, 0] ** 2 + 3 * x[0 ,1]\n",
    "y[0, 1] = x[0, 1] ** 2 + 2 * x[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# y.backward(torch.Tensor([[1, 1]]))\n",
    "# print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 生成y1对x的梯度\n",
    "y.backward(torch.Tensor([[1, 0]]),retain_graph=True)\n",
    "J[0]=x.grad\n",
    "# 梯度是累加的，故需要对x的梯度清零\n",
    "x.grad = torch.zeros_like(x.grad)\n",
    "# 生成y2对x的梯度\n",
    "y.backward(torch.Tensor([[0, 1]]))\n",
    "J[1]=x.grad\n",
    "# 显示jacobian矩阵的值\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>2.6 使用Numpy实现机器学习</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(100) \n",
    "x = np.linspace(-1, 1, 100).reshape(100,1) \n",
    "y = 3*np.power(x, 2) +2+ 0.2*np.random.rand(x.size).reshape(100,1)  \n",
    "\n",
    "# 画图\n",
    "plt.scatter(x, y)\n",
    "plt.show()\n",
    "\n",
    "# 随机初始化参数\n",
    "w1 = np.random.rand(1,1)\n",
    "b1 = np.random.rand(1,1) \n",
    "\n",
    "lr =0.001 # 学习率\n",
    "\n",
    "for i in range(800):\n",
    "    # 前向传播\n",
    "    y_pred = np.power(x,2)*w1 + b1\n",
    "    # 定义损失函数\n",
    "    loss = 0.5 * (y_pred - y) ** 2\n",
    "    loss = loss.sum()\n",
    "    #计算梯度\n",
    "    grad_w=np.sum((y_pred - y)*np.power(x,2))\n",
    "    grad_b=np.sum((y_pred - y))\n",
    "    #使用梯度下降法，是loss最小\n",
    "    w1 -= lr * grad_w\n",
    "    b1 -= lr * grad_b\n",
    "\n",
    "plt.plot(x, y_pred,'r-',label='predict')\n",
    "plt.scatter(x, y,color='blue',marker='o',label='true') # true data\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(2,6)  \n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(w1,b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.7 使用Tensor及antograd实现机器学习</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t.manual_seed(100) \n",
    "dtype = t.float\n",
    "# 生成x坐标数据，x为tenor，形状为100x1\n",
    "x = t.unsqueeze(torch.linspace(-1, 1, 100), dim=1) \n",
    "# 生成y坐标数据，y为tenor，形状为100x1，另加上一些噪音\n",
    "y = 3*x.pow(2) +2+ 0.2*torch.rand(x.size())                 \n",
    "\n",
    "# 画图，把tensor数据转换为numpy数据\n",
    "plt.scatter(x.numpy(), y.numpy())\n",
    "plt.show()\n",
    "\n",
    "# 随机初始化参数，参数w，b为需要学习的，故需requires_grad=True\n",
    "w = t.randn(1,1, dtype=dtype,requires_grad=True)\n",
    "b = t.zeros(1,1, dtype=dtype, requires_grad=True) \n",
    "\n",
    "lr =0.001 # 学习率\n",
    "\n",
    "for ii in range(800):\n",
    "    # forward：计算loss\n",
    "    y_pred = x.pow(2).mm(w) + b\n",
    "    loss = 0.5 * (y_pred - y) ** 2\n",
    "    loss = loss.sum()\n",
    "    \n",
    "    # backward：自动计算梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 手动更新参数，需要用torch.no_grad()更新参数\n",
    "    with t.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    # 梯度清零\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "plt.plot(x.numpy(), y_pred.detach().numpy(),'r-',label='predict')#predict\n",
    "plt.scatter(x.numpy(), y.numpy(),color='blue',marker='o',label='true') # true data\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(2,6)  \n",
    "plt.legend()\n",
    "plt.show()\n",
    "        \n",
    "print(w, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.8 使用TensorFlow架构</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 生成训练数据\n",
    "np.random.seed(100) \n",
    "x = np.linspace(-1, 1, 100).reshape(100,1) \n",
    "y = 3*np.power(x, 2) +2+ 0.2*np.random.rand(x.size).reshape(100,1)  \n",
    "\n",
    "\n",
    "# 创建两个占位符，分别用来存放输入数据x和目标值y\n",
    "# 运行计算图时，导入数据.\n",
    "x1 = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "y1 = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "# 创建权重变量w和b，并用随机值初始化.\n",
    "# TensorFlow 的变量在整个计算图保存其值.\n",
    "w = tf.Variable(tf.random_uniform([1], 0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "\n",
    "# 前向传播，计算预测值.\n",
    "y_pred = np.power(x,2)*w + b\n",
    "\n",
    "# 计算损失值\n",
    "loss=tf.reduce_mean(tf.square(y-y_pred)) \n",
    "\n",
    "# 计算有关参数w、b关于损失函数的梯度.\n",
    "grad_w, grad_b = tf.gradients(loss, [w, b])\n",
    "\n",
    "# 用梯度下降法更新参数.\n",
    "# 执行计算图时给 new_w1 和new_w2 赋值\n",
    "# 对TensorFlow 来说，更新参数是计算图的一部分内容\n",
    "# 而PyTorch，这部分是属于计算图之外.\n",
    "learning_rate = 0.01\n",
    "new_w = w.assign(w - learning_rate * grad_w)\n",
    "new_b = b.assign(b - learning_rate * grad_b)\n",
    "\n",
    "# 已构建计算图, 接下来创建TensorFlow session，准备执行计算图.\n",
    "with tf.Session() as sess:\n",
    "    # 执行之前需要初始化变量w、b\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    for step in range(2000):\n",
    "        # 循环执行计算图. 每次需要把x1,y1赋给x和y.\n",
    "        # 每次执行计算图时，需要计算关于new_w和new_b的损失值,\n",
    "        # 返回numpy多维数组\n",
    "        loss_value, v_w, v_b = sess.run([loss, new_w, new_b],\n",
    "                                    feed_dict={x1: x, y1: y})\n",
    "        if  step%200==0:  #每200次打印一次训练结果\n",
    "            print(\"损失值、权重、偏移量分别为{:.4f},{},{}\".format(loss_value,v_w,v_b))\n",
    "# 可视化结果            \n",
    "plt.figure() \n",
    "plt.scatter(x,y)\n",
    "plt.plot (x, v_b + v_w*x**2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}